#!/usr/bin/env python3
"""
MCP QA Orchestration System - Phase 2 Demo
===========================================

This demo script showcases the fully implemented LangChain agents:
1. ScenarioBuilderAgent - Generates BDD scenarios from codebase
2. ReviewerAgent - Reviews and improves BDD scenarios
3. TestCoderAgent - Creates Python test code from scenarios
4. CodeReviewerAgent - Reviews generated test code

Requirements:
- pip install -r requirements.txt
- Set OPENAI_API_KEY environment variable

Usage:
python demo.py
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from typing import Dict, Any

# Import the MCP system
sys.path.insert(0, os.path.dirname(__file__))
from mcp_qa_orchestrator import (
    MCPOrchestrator,
    MCPConfig,
    LANGCHAIN_AVAILABLE
)

# Ensure proper environment setup
def check_environment():
    """Check if environment is properly configured"""
    print("ðŸ” Checking environment setup...")

    # Check LangChain availability
    if not LANGCHAIN_AVAILABLE:
        print("âš ï¸  LangChain dependencies not available!")
        print("   Install with: pip install -r requirements.txt")
        return False

    # Check API key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸  OPENAI_API_KEY environment variable not set!")
        print("   Set with: export OPENAI_API_KEY=your-api-key-here")
        return False

    print("âœ… Environment ready!")
    return True

def create_sample_codebase() -> Dict[str, Any]:
    """Create a comprehensive sample codebase for demonstration"""
    return {
        "project_name": "SmartHomeController",
        "language": "python",
        "architecture": "Microservices with MQTT messaging",
        "description": "A smart home automation controller with device management and voice control",
        "critical_paths": [
            "device_discovery",
            "voice_command_processing",
            "security_authentication",
            "device_state_management"
        ],
        "integration_points": [
            "mqtt_broker",
            "device_registry",
            "voice_stt_service",
            "voice_tts_service"
        ],
        "error_scenarios": [
            "network_disconnection",
            "invalid_device_commands",
            "authentication_failures",
            "voice_recognition_errors"
        ],
        "structure": {
            "src": {
                "controller": {
                    "main.py": "Main controller application",
                    "device_manager.py": "Device registry and management",
                    "voice_processor.py": "Voice command processing",
                    "security_manager.py": "Authentication and authorization",
                    "mqtt_client.py": "MQTT communication handler",
                    "config_manager.py": "Configuration management"
                },
                "services": {
                    "stt_service.py": "Speech-to-text service",
                    "tts_service.py": "Text-to-speech service",
                    "device_service.py": "Generic device control",
                    "automation_service.py": "Home automation rules"
                },
                "utils": {
                    "logger.py": "Centralized logging",
                    "validators.py": "Data validation utilities",
                    "exceptions.py": "Custom exception classes"
                }
            },
            "tests": {
                "unit": {
                    "test_device_manager.py": "Device manager unit tests",
                    "test_voice_processor.py": "Voice processing unit tests",
                    "test_security.py": "Security functionality tests"
                },
                "integration": {
                    "test_mqtt_integration.py": "MQTT integration tests",
                    "test_voice_pipeline.py": "Voice processing pipeline tests"
                },
                "e2e": {
                    "test_complete_workflow.py": "End-to-end test scenarios"
                }
            },
            "docs": {
                "api_reference.md": "API documentation",
                "setup_guide.md": "Installation and setup",
                "user_manual.md": "User guide"
            },
            "config": {
                "devices.json": "Device configuration",
                "mqtt_config.json": "MQTT broker settings",
                "security_config.json": "Security settings"
            }
        }
    }

async def run_demo():
    """Run the complete MCP QA orchestration demo"""
    print("\nðŸš€ MCP QA Orchestration System - Phase 2 Demo")
    print("=" * 60)

    # Check environment
    if not check_environment():
        print("\nâŒ Demo cannot run due to environment issues.")
        print("Please install dependencies and set API key, then try again.")
        return

    # Create sample codebase
    print("\nðŸ“Š Creating sample codebase...")
    codebase = create_sample_codebase()
    print(f"âœ… Sample project: {codebase['project_name']}")
    print(f"ðŸ“ Files analyzed: {len(json.dumps(codebase['structure']))} chars of structure")

    # Create MCP configuration
    print("\nâš™ï¸  Configuring MCP orchestrator...")
    config = MCPConfig(
        bdd_threshold=8.0,      # Require 80%+ BDD coverage
        quality_threshold=7.5,  # Require 75%+ test quality
        max_iterations=2,       # Allow up to 2 improvement iterations
        max_bugs=2,            # Maximum 2 bugs allowed
        llm_provider="openai",
        api_key=os.getenv("OPENAI_API_KEY"),
        timeout=60,            # 60 second timeout per agent
        retry_attempts=2       # 2 retry attempts per agent
    )
    print(f"âœ… Configuration: BDDâ‰¥{config.bdd_threshold}, Qualityâ‰¥{config.quality_threshold}")

    # Initialize orchestrator
    print("\nðŸ¤– Initializing LangChain agents...")
    orchestrator = MCPOrchestrator(config)

    try:
        # Display available agents
        print(f"ðŸŽ¯ Available agents: {list(orchestrator.agents.keys())}")

        # Start orchestration
        print("\nðŸŽ¬ Starting MCP orchestration...")
        start_time = datetime.now()

        result = await orchestrator.spin_up_mcp(codebase)

        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()

        # Display results
        print(f"\nðŸŽ‰ Orchestration completed in {duration:.1f} seconds!")
        print("=" * 60)

        # Summary
        status = result.get("summary", {})
        quality = result.get("quality_assessment", {})

        print(f"ðŸ“Š Final Status: {result.get('status', 'unknown').upper()}")
        print("ðŸ“ˆ Quality Scores:")
        print(f"   â€¢ BDD Coverage: {result.get('results', {}).get('bdd_scenarios', {}).get('coverage_score', 0):.1f}/10")
        print(f"   â€¢ Test Quality: {result.get('results', {}).get('test_files', {}).get('quality_score', 0):.1f}/10")
        print(f"   â€¢ Overall Score: {quality.get('overall_score', 0):.1f}/10")
        print(f"   â€¢ Thresholds Met: {quality.get('thresholds_met', False)}")
        print(f"   â€¢ Iterations: {status.get('total_iterations', 0)}")

        # Agents executed
        agents_executed = result.get("results", {}).get("agents_executed", [])
        print(f"ðŸ¤– Agents Executed: {len(agents_executed)}")
        for agent in agents_executed:
            print(f"   â€¢ {agent.replace('_', ' ').title()}")

        # Show sample BDD scenarios if available
        bdd_scenarios = result.get("results", {}).get("bdd_scenarios", {})
        if bdd_scenarios.get("features"):
            print("
ðŸ… Sample BDD Feature:"            feature = bdd_scenarios["features"][0]
            print(f"Feature: {feature['name']}")
            print(f"Description: {feature['description']}")
            if feature.get("scenarios"):
                scenario = feature["scenarios"][0]
                print(".1f")
                print("ðŸ“ Steps:")
                for step in scenario.get("steps", []):
                    print(f"   {step}")

        # Show test generation if available
        test_files = result.get("results", {}).get("test_files", {})
        if test_files.get("files"):
            print("
ðŸ“ Generated Test File:"            test_file = test_files["files"][0]
            lines = test_file.get("content", "").split("\n")[:10]  # First 10 lines
            print(f"Test File: {test_file.get('path', 'unknown')}")
            print("Content Preview:")
            for line in lines:
                if line.strip():
                    print(f"   {line}")

        # Show recommendations
        recommendations = quality.get("recommendations", [])
        if recommendations:
            print("
ðŸ’¡ Recommendations:"            for rec in recommendations[:3]:  # Show first 3
                print(f"   â€¢ {rec}")

        print("\nðŸŽ¯ System Features Demonstrated:")
        print("   âœ… LangChain LLM Integration")
        print("   âœ… Structured Output Processing")
        print("   âœ… BDD Scenario Generation")
        print("   âœ… Python Test Code Creation")
        print("   âœ… Code Quality Review")
        print("   âœ… Iterative Quality Improvement")
        print("   âœ… Error Recovery & Retry Logic")
        print("   âœ… Comprehensive Reporting")

    except Exception as e:
        print(f"\nâŒ Demo failed: {str(e)}")
        print("ðŸ’¡ This might be due to:")
        print("   â€¢ API key issues")
        print("   â€¢ Network connectivity")
        print("   â€¢ LangChain configuration")
        print("   â€¢ OpenAI API limits or errors")
        print("\nðŸ”§ Troubleshooting:")
        print("   1. Check your OpenAI API key is valid")
        print("   2. Verify internet connection")
        print("   3. Ensure LangChain dependencies are installed")
        print("   4. Check OpenAI account has sufficient credits")
        print("ðŸ“ˆ Quality Scores:")
        print(f"   â€¢ BDD Coverage: {result.get('results', {}).get('bdd_scenarios', {}).get('coverage_score', 0):.1f}/10")
        print(f"   â€¢ Test Quality: {result.get('results', {}).get('test_files', {}).get('quality_score', 0):.1f}/10")
        print(f"   â€¢ Overall Score: {quality.get('overall_score', 0):.1f}/10")
        print(f"   â€¢ Thresholds Met: {quality.get('thresholds_met', False)}")
        print(f"   â€¢ Iterations: {status.get('total_iterations', 0)}")

        # Agents executed
        agents_executed = result.get("results", {}).get("agents_executed", [])
        print(f"ðŸ¤– Agents Executed: {len(agents_executed)}")
        for agent in agents_executed:
            print(f"   â€¢ {agent.replace('_', ' ').title()}")

        # Show sample BDD scenarios if available
        bdd_scenarios = result.get("results", {}).get("bdd_scenarios", {})
        if bdd_scenarios.get("features"):
            print("\nðŸ… Sample BDD Feature:")
            feature = bdd_scenarios["features"][0]
            print(f"Feature: {feature['name']}")
            print(f"Description: {feature['description']}")
            if feature.get("scenarios"):
                scenario = feature["scenarios"][0]
                print(".1f")
                print("ðŸ“ Steps:")
                for step in scenario.get("steps", []):
                    print(f"   {step}")

        # Show test generation if available
        test_files = result.get("results", {}).get("test_files", {})
        if test_files.get("files"):
            print("\nðŸ“ Generated Test File:")
            test_file = test_files["files"][0]
            lines = test_file.get("content", "").split("\n")[:10]  # First 10 lines
            print(f"Test File: {test_file.get('path', 'unknown')}")
            print("Content Preview:")
            for line in lines:
                if line.strip():
                    print(f"   {line}")

        # Show recommendations
        recommendations = quality.get("recommendations", [])
        if recommendations:
            print("\nðŸ’¡ Recommendations:")
            for rec in recommendations[:3]:  # Show first 3
                print(f"   â€¢ {rec}")

        print("\nðŸŽ¯ System Features Demonstrated:")
        print("   âœ… LangChain LLM Integration")
        print("   âœ… Structured Output Processing")
        print("   âœ… BDD Scenario Generation")
        print("   âœ… Python Test Code Creation")
        print("   âœ… Code Quality Review")
        print("   âœ… Iterative Quality Improvement")
        print("   âœ… Error Recovery & Retry Logic")
        print("   âœ… Comprehensive Reporting")

    except Exception as e:
        print(f"\nâŒ Demo failed: {str(e)}")
        print("ðŸ’¡ This might be due to:")
        print("   â€¢ API key issues")
        print("   â€¢ Network connectivity")
        print("   â€¢ LangChain configuration")
        print("   â€¢ OpenAI API limits or errors")
        print("\nðŸ”§ Troubleshooting:")
        print("   1. Check your OpenAI API key is valid")
        print("   2. Verify internet connection")
        print("   3. Ensure LangChain dependencies are installed")
        print("   4. Check OpenAI account has sufficient credits")
        print("\nðŸ”„ Demo complete - check for any error messages above")
        print(f"   â€¢ Test Quality: {result.get('results', {}).get('test_files', {}).get('quality_score', 0):.1f}/10")
        print(f"   â€¢ Overall Score: {quality.get('overall_score', 0):.1f}/10")
        print(f"   â€¢ Thresholds Met: {quality.get('thresholds_met', False)}")
        print(f"   â€¢ Iterations: {status.get('total_iterations', 0)}")

        # Agents executed
        agents_executed = result.get("results", {}).get("agents_executed", [])
        print(f"ðŸ¤– Agents Executed: {len(agents_executed)}")
        for agent in agents_executed:
            print(f"   â€¢ {agent.replace('_', ' ').title()}")

        # Show sample BDD scenarios if available
        bdd_scenarios = result.get("results", {}).get("bdd_scenarios", {})
        if bdd_scenarios.get("features"):
            print("
ðŸ… Sample BDD Feature:"            feature = bdd_scenarios["features"][0]
            print(f"Feature: {feature['name']}")
            print(f"Description: {feature['description']}")
            if feature.get("scenarios"):
                scenario = feature["scenarios"][0]
                print(".1f")
                print("ðŸ“ Steps:")
                for step in scenario.get("steps", []):
                    print(f"   {step}")

        # Show test generation if available
        test_files = result.get("results", {}).get("test_files", {})
        if test_files.get("files"):
            print("
ðŸ“ Generated Test File:"            test_file = test_files["files"][0]
            lines = test_file.get("content", "").split("\n")[:10]  # First 10 lines
            print(f"Test File: {test_file.get('path', 'unknown')}")
            print("Content Preview:")
            for line in lines:
                if line.strip():
                    print(f"   {line}")

        # Show recommendations
        recommendations = quality.get("recommendations", [])
        if recommendations:
            print("
ðŸ’¡ Recommendations:"            for rec in recommendations[:3]:  # Show first 3
                print(f"   â€¢ {rec}")

        print("\nðŸŽ¯ System Features Demonstrated:")
        print("   âœ… LangChain LLM Integration")
        print("   âœ… Structured Output Processing")
        print("   âœ… BDD Scenario Generation")
        print("   âœ… Python Test Code Creation")
        print("   âœ… Code Quality Review")
        print("   âœ… Iterative Quality Improvement")
        print("   âœ… Error Recovery & Retry Logic")
        print("   âœ… Comprehensive Reporting")

    except Exception as e:
        print(f"\nâŒ Demo failed: {str(e)}")
        print("ðŸ’¡ This might be due to:")
        print("   â€¢ API key issues")
        print("   â€¢ Network connectivity")
        print("   â€¢ LangChain configuration")
        print("   â€¢ OpenAI API limits or errors")
        print("\nðŸ”§ Troubleshooting:")
        print("   1. Check your OpenAI API key is valid")
        print("   2. Verify internet connection")
        print("   3. Ensure LangChain dependencies are installed")
        print("   4. Check OpenAI account has sufficient credits")

if __name__ == "__main__":
    print("ðŸ¤– MCP QA Orchestration System - Phase 2 Demo\n")

    if len(sys.argv) > 1 and sys.argv[1] == "--help":
        print("Usage: python demo.py")
        print("\nRequirements:")
        print("â€¢ Python 3.8+")
        print("â€¢ LangChain dependencies (pip install -r requirements.txt)")
        print("â€¢ OpenAI API key (export OPENAI_API_KEY=your-key)")
        print("\nThis demo showcases:")
        print("â€¢ LangChain-powered AI agents")
        print("â€¢ BDD scenario generation")
        print("â€¢ Python test code creation")
        print("â€¢ Quality assessment and iteration")
        sys.exit(0)

    # Run the demo
    asyncio.run(run_demo())
