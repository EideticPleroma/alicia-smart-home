# Cline Phase 3 Implementation Prompt

## ğŸ¯ Task: Create Configuration and Documentation

**Agent**: Cline (grok-code-fast-1 model)  
**Phase**: 3 of 4  
**Priority**: Usability and Deployment  
**Dependencies**: Phase 1 and 2 completed  

## ğŸ“‹ Implementation Requirements

### Files to Create

1. **`requirements.txt`** - Python dependencies
2. **`config.json`** - Configuration file
3. **`README.md`** - Comprehensive documentation
4. **`example_usage.py`** - Usage examples
5. **`docker-compose.yml`** - Docker deployment
6. **`.env.example`** - Environment variables template

## ğŸ“¦ 1. Requirements File

### File: `requirements.txt`

```txt
# Core dependencies
langchain>=0.1.0
langchain-openai>=0.1.0
openai>=1.0.0
pydantic>=2.0.0

# Testing dependencies
pytest>=7.0.0
pytest-asyncio>=0.21.0
pytest-mock>=3.10.0
pytest-cov>=4.0.0

# Async and HTTP
aiohttp>=3.8.0
asyncio-mqtt>=0.16.0

# Data processing
pandas>=1.5.0
numpy>=1.24.0

# File operations
python-multipart>=0.0.5
python-dotenv>=1.0.0

# Logging and monitoring
structlog>=23.0.0
rich>=13.0.0

# Development dependencies
black>=23.0.0
flake8>=6.0.0
mypy>=1.0.0
pre-commit>=3.0.0

# Optional: Grok API support
grok-api>=0.1.0
```

## âš™ï¸ 2. Configuration File

### File: `config.json`

```json
{
  "mcp_config": {
    "bdd_threshold": 9.0,
    "quality_threshold": 8.0,
    "max_iterations": 3,
    "max_bugs": 3,
    "llm_provider": "openai",
    "base_url": null,
    "timeout": 30,
    "retry_attempts": 3
  },
  "agents": {
    "scenario_builder": {
      "model": "gpt-4",
      "temperature": 0.7,
      "max_tokens": 3000,
      "timeout": 30,
      "retry_attempts": 3
    },
    "reviewer": {
      "model": "gpt-4",
      "temperature": 0.3,
      "max_tokens": 2000,
      "timeout": 30,
      "retry_attempts": 3
    },
    "test_coder": {
      "model": "gpt-4",
      "temperature": 0.5,
      "max_tokens": 4000,
      "timeout": 45,
      "retry_attempts": 3
    },
    "code_reviewer": {
      "model": "gpt-4",
      "temperature": 0.2,
      "max_tokens": 3000,
      "timeout": 30,
      "retry_attempts": 3
    }
  },
  "logging": {
    "level": "INFO",
    "format": "%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    "file": "logs/mcp_orchestrator.log",
    "max_size": "10MB",
    "backup_count": 5
  },
  "output": {
    "base_dir": "output",
    "include_timestamps": true,
    "compress_results": true,
    "save_intermediate": true
  },
  "quality_gates": {
    "minimum_bdd_score": 8.0,
    "minimum_test_score": 7.0,
    "minimum_review_score": 7.0,
    "maximum_bugs": 5,
    "coverage_threshold": 80
  }
}
```

## ğŸ“š 3. Documentation File

### File: `README.md`

```markdown
# MCP QA Orchestration System

A comprehensive Python-based orchestration system for automated QA testing using LangChain for agent chaining. The system generates BDD scenarios, creates Python test skeletons, and provides detailed quality reports.

## ğŸš€ Features

- **Four Specialized AI Agents**: Scenario Builder, Reviewer, Test-Coder, and Code-Reviewer
- **LangChain Integration**: Full LangChain agent framework implementation
- **Async Execution**: Non-blocking agent execution pipeline
- **Quality Thresholds**: Configurable quality thresholds for different use cases
- **JSON Communication**: Structured data exchange between agents
- **Test Packaging**: Automatic ZIP packaging of generated test files
- **Multiple LLM Support**: Support for OpenAI and Grok APIs
- **Comprehensive Testing**: Complete test suite with core functionality validation

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Scenario Builderâ”‚â”€â”€â”€â–¶â”‚    Reviewer     â”‚â”€â”€â”€â–¶â”‚   Test-Coder    â”‚â”€â”€â”€â–¶â”‚  Code-Reviewer  â”‚
â”‚     Agent       â”‚    â”‚     Agent       â”‚    â”‚     Agent       â”‚    â”‚     Agent       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚                       â”‚
         â–¼                       â–¼                       â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MCP Orchestrator                                         â”‚
â”‚                    (Master Control Program)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- Python 3.11+
- OpenAI API key or Grok API key
- 4GB+ RAM recommended
- 1GB+ disk space for outputs

## ğŸ› ï¸ Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd mcp-qa-orchestrator
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**:
   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

4. **Run tests**:
   ```bash
   python simple_test.py
   python test_mcp.py
   ```

## âš™ï¸ Configuration

### Environment Variables

Create a `.env` file with the following variables:

```env
# LLM Configuration
OPENAI_API_KEY=your_openai_api_key_here
GROK_API_KEY=your_grok_api_key_here
LLM_PROVIDER=openai
LLM_BASE_URL=

# MCP Configuration
BDD_THRESHOLD=9.0
QUALITY_THRESHOLD=8.0
MAX_ITERATIONS=3
MAX_BUGS=3

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/mcp_orchestrator.log

# Output
OUTPUT_DIR=output
INCLUDE_TIMESTAMPS=true
COMPRESS_RESULTS=true
```

### Configuration File

Edit `config.json` to customize agent behavior, quality thresholds, and output settings.

## ğŸš€ Usage

### Basic Usage

```python
import asyncio
from mcp_qa_orchestrator import MCPOrchestrator, MCPConfig

async def main():
    # Create configuration
    config = MCPConfig(
        bdd_threshold=9.0,
        quality_threshold=8.0,
        max_iterations=3,
        llm_provider="openai",
        api_key="your-api-key"
    )
    
    # Create orchestrator
    mcp = MCPOrchestrator(config)
    
    # Define codebase snapshot
    codebase = {
        "project_name": "My Project",
        "architecture": "Microservices",
        "structure": {
            "services": ["auth", "api", "db"],
            "frontend": "React",
            "database": "PostgreSQL"
        },
        "critical_paths": ["user_auth", "data_processing"],
        "integration_points": ["external_api"],
        "error_scenarios": ["network_failure", "auth_failure"]
    }
    
    # Run orchestration
    result = await mcp.spin_up_mcp(codebase)
    
    # Print results
    print(json.dumps(result, indent=2))

# Run the example
asyncio.run(main())
```

### Advanced Usage

```python
import asyncio
from mcp_qa_orchestrator import MCPOrchestrator, MCPConfig
from mcp_qa_orchestrator import ScenarioBuilderAgent, ReviewerAgent

async def advanced_example():
    # Custom configuration
    config = MCPConfig(
        bdd_threshold=8.5,
        quality_threshold=7.5,
        max_iterations=5,
        llm_provider="grok",
        api_key="your-grok-key",
        base_url="https://api.grok.com/v1"
    )
    
    # Create orchestrator
    mcp = MCPOrchestrator(config)
    
    # Run individual agents
    scenario_agent = mcp.agents["scenario_builder"]
    bdd_result = await scenario_agent.run({"codebase": codebase})
    
    # Continue with other agents...
```

## ğŸ§ª Testing

### Run All Tests

```bash
# Core functionality tests
python simple_test.py

# Full test suite with mocking
python test_mcp.py

# Run with coverage
pytest --cov=mcp_qa_orchestrator --cov-report=html
```

### Test Categories

1. **Unit Tests**: Individual component testing
2. **Integration Tests**: Agent pipeline testing
3. **Performance Tests**: Speed and memory usage
4. **Error Tests**: Error handling and recovery

## ğŸ“Š Output Format

The system generates comprehensive reports in JSON format:

```json
{
  "status": "success",
  "summary": {
    "bdd_coverage_score": 9.2,
    "test_quality_score": 8.7,
    "review_score": 8.9,
    "total_iterations": 2,
    "final_state": "completed"
  },
  "bdd_scenarios": {
    "features": [
      {
        "name": "User Authentication",
        "description": "User login and authentication",
        "scenarios": [
          {
            "name": "Successful login",
            "steps": [
              "Given a valid user",
              "When they enter credentials",
              "Then they should be logged in"
            ],
            "tags": ["@smoke", "@auth"],
            "priority": "high"
          }
        ]
      }
    ],
    "coverage_score": 9.2
  },
  "test_files": {
    "files": [
      {
        "path": "tests/test_auth.py",
        "content": "def test_login(): ...",
        "test_count": 5,
        "coverage_estimate": 85.0
      }
    ],
    "quality_score": 8.7,
    "packaged_files": "base64_encoded_zip_content"
  },
  "review_results": {
    "score": 8.9,
    "green": true,
    "bugs": [],
    "recommendations": [
      "Add more edge case testing",
      "Improve error message validation"
    ]
  }
}
```

## ğŸ”§ Configuration Options

### MCP Configuration

| Option | Default | Description |
|--------|---------|-------------|
| `bdd_threshold` | 9.0 | Minimum BDD coverage score |
| `quality_threshold` | 8.0 | Minimum test quality score |
| `max_iterations` | 3 | Maximum orchestration iterations |
| `max_bugs` | 3 | Maximum allowed bugs |
| `llm_provider` | "openai" | LLM provider (openai/grok) |
| `timeout` | 30 | Request timeout in seconds |
| `retry_attempts` | 3 | Number of retry attempts |

### Agent Configuration

Each agent can be configured with:
- **Model**: LLM model to use
- **Temperature**: Creativity level (0.0-1.0)
- **Max Tokens**: Maximum response length
- **Timeout**: Request timeout
- **Retry Attempts**: Number of retries

## ğŸ³ Docker Deployment

### Docker Compose

```yaml
version: '3.8'
services:
  mcp-orchestrator:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LOG_LEVEL=INFO
    volumes:
      - ./output:/app/output
      - ./logs:/app/logs
    restart: unless-stopped
```

### Dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["python", "example_usage.py"]
```

## ğŸ” Troubleshooting

### Common Issues

1. **API Key Errors**:
   - Ensure API key is set in environment variables
   - Check API key validity and permissions

2. **Timeout Errors**:
   - Increase timeout values in configuration
   - Check network connectivity

3. **Memory Issues**:
   - Reduce max_tokens for agents
   - Increase system memory

4. **Quality Threshold Failures**:
   - Adjust thresholds in configuration
   - Review generated content quality

### Debug Mode

Enable debug logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“ˆ Performance

### Benchmarks

- **Agent Execution**: 15-30 seconds per agent
- **Full Pipeline**: 1-3 minutes total
- **Memory Usage**: 200-500MB
- **Output Size**: 1-10MB per run

### Optimization Tips

1. **Reduce Max Tokens**: Lower token limits for faster responses
2. **Use Smaller Models**: GPT-3.5-turbo for faster execution
3. **Parallel Processing**: Run agents in parallel where possible
4. **Caching**: Cache LLM responses for repeated inputs

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Support

For support and questions:
- Create an issue on GitHub
- Check the troubleshooting section
- Review the configuration options

## ğŸ”„ Changelog

### Version 1.0.0
- Initial release
- Four specialized agents
- LangChain integration
- Comprehensive testing
- Docker support

---

**Ready for production use!** ğŸš€
```

## ğŸ“ 4. Example Usage File

### File: `example_usage.py`

```python
#!/usr/bin/env python3
"""
Example usage of the MCP QA Orchestration System
Demonstrates various usage patterns and configurations
"""

import asyncio
import json
import os
from datetime import datetime
from mcp_qa_orchestrator import MCPOrchestrator, MCPConfig

# Example 1: Basic Usage
async def basic_example():
    """Basic usage example"""
    print("ğŸš€ Basic Usage Example")
    print("=" * 50)
    
    # Create configuration
    config = MCPConfig(
        bdd_threshold=9.0,
        quality_threshold=8.0,
        max_iterations=3,
        llm_provider="openai",
        api_key=os.getenv("OPENAI_API_KEY")
    )
    
    # Create orchestrator
    mcp = MCPOrchestrator(config)
    
    # Define codebase snapshot
    codebase = {
        "project_name": "E-commerce API",
        "architecture": "Microservices",
        "structure": {
            "services": ["auth", "products", "orders", "payments"],
            "frontend": "React",
            "database": "PostgreSQL",
            "cache": "Redis"
        },
        "critical_paths": [
            "user_authentication",
            "product_catalog",
            "order_processing",
            "payment_processing"
        ],
        "integration_points": [
            "payment_gateway",
            "inventory_system",
            "notification_service"
        ],
        "error_scenarios": [
            "payment_failure",
            "inventory_unavailable",
            "network_timeout",
            "authentication_failure"
        ]
    }
    
    try:
        # Run orchestration
        result = await mcp.spin_up_mcp(codebase)
        
        # Print summary
        print(f"Status: {result['status']}")
        print(f"BDD Score: {result['summary']['bdd_coverage_score']}")
        print(f"Test Score: {result['summary']['test_quality_score']}")
        print(f"Iterations: {result['summary']['total_iterations']}")
        
        return result
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        return None

# Example 2: Advanced Configuration
async def advanced_example():
    """Advanced configuration example"""
    print("\nğŸ”§ Advanced Configuration Example")
    print("=" * 50)
    
    # Custom configuration
    config = MCPConfig(
        bdd_threshold=8.5,
        quality_threshold=7.5,
        max_iterations=5,
        max_bugs=5,
        llm_provider="openai",
        api_key=os.getenv("OPENAI_API_KEY"),
        timeout=45,
        retry_attempts=5
    )
    
    # Create orchestrator
    mcp = MCPOrchestrator(config)
    
    # Complex codebase
    codebase = {
        "project_name": "Alicia Smart Home AI",
        "architecture": "Microservices with MQTT",
        "structure": {
            "services": [
                "voice-router", "stt-service", "ai-service", "tts-service",
                "device-manager", "ha-bridge", "sonos-service", "security-gateway"
            ],
            "message_bus": "MQTT",
            "frontend": "React/TypeScript",
            "database": "PostgreSQL",
            "cache": "Redis"
        },
        "critical_paths": [
            "voice_command_processing",
            "device_control",
            "home_assistant_integration",
            "security_authentication"
        ],
        "integration_points": [
            "home_assistant_api",
            "sonos_speakers",
            "mqtt_broker",
            "external_apis"
        ],
        "error_scenarios": [
            "mqtt_connection_failure",
            "device_unavailable",
            "voice_recognition_failure",
            "security_breach"
        ]
    }
    
    try:
        # Run orchestration
        result = await mcp.spin_up_mcp(codebase)
        
        # Analyze results
        print(f"Status: {result['status']}")
        print(f"BDD Features: {len(result['bdd_scenarios']['features'])}")
        print(f"Test Files: {len(result['test_files']['files'])}")
        print(f"Quality Score: {result['summary']['test_quality_score']}")
        
        return result
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        return None

# Example 3: Individual Agent Usage
async def individual_agent_example():
    """Individual agent usage example"""
    print("\nğŸ¤– Individual Agent Example")
    print("=" * 50)
    
    config = MCPConfig(
        llm_provider="openai",
        api_key=os.getenv("OPENAI_API_KEY")
    )
    
    mcp = MCPOrchestrator(config)
    
    # Simple codebase
    codebase = {
        "project_name": "Simple API",
        "structure": {"endpoints": ["/users", "/products"]},
        "critical_paths": ["user_management"]
    }
    
    try:
        # Run individual agents
        scenario_agent = mcp.agents["scenario_builder"]
        bdd_result = await scenario_agent.run({"codebase": codebase})
        
        print(f"BDD Features: {len(bdd_result['features'])}")
        print(f"Coverage Score: {bdd_result['coverage_score']}")
        
        # Continue with other agents...
        reviewer_agent = mcp.agents["reviewer"]
        review_result = await reviewer_agent.run({"bdd_scenarios": bdd_result})
        
        print(f"Review Score: {review_result['coverage_score']}")
        
        return {"bdd": bdd_result, "review": review_result}
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        return None

# Example 4: Error Handling
async def error_handling_example():
    """Error handling example"""
    print("\nâš ï¸ Error Handling Example")
    print("=" * 50)
    
    # Invalid configuration
    try:
        config = MCPConfig(
            llm_provider="invalid_provider",
            api_key="invalid_key"
        )
        print("âŒ Should have failed with invalid provider")
    except ValueError as e:
        print(f"âœ… Correctly caught error: {e}")
    
    # Missing API key
    try:
        config = MCPConfig(llm_provider="openai")
        print("âŒ Should have failed with missing API key")
    except ValueError as e:
        print(f"âœ… Correctly caught error: {e}")
    
    # Invalid codebase
    try:
        config = MCPConfig(
            llm_provider="openai",
            api_key=os.getenv("OPENAI_API_KEY")
        )
        mcp = MCPOrchestrator(config)
        
        # Invalid codebase (missing required fields)
        invalid_codebase = {"project_name": "Test"}
        result = await mcp.spin_up_mcp(invalid_codebase)
        print("âŒ Should have failed with invalid codebase")
    except ValueError as e:
        print(f"âœ… Correctly caught error: {e}")

# Example 5: Performance Testing
async def performance_example():
    """Performance testing example"""
    print("\nâš¡ Performance Testing Example")
    print("=" * 50)
    
    config = MCPConfig(
        llm_provider="openai",
        api_key=os.getenv("OPENAI_API_KEY"),
        timeout=30
    )
    
    mcp = MCPOrchestrator(config)
    
    # Simple codebase for performance testing
    codebase = {
        "project_name": "Performance Test",
        "structure": {"services": ["api"]},
        "critical_paths": ["basic_functionality"]
    }
    
    try:
        start_time = datetime.now()
        result = await mcp.spin_up_mcp(codebase)
        end_time = datetime.now()
        
        duration = (end_time - start_time).total_seconds()
        print(f"â±ï¸ Execution time: {duration:.2f} seconds")
        print(f"ğŸ“Š Status: {result['status']}")
        print(f"ğŸ”„ Iterations: {result['summary']['total_iterations']}")
        
        return result
        
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        return None

# Main execution
async def main():
    """Run all examples"""
    print("ğŸ¯ MCP QA Orchestration System - Examples")
    print("=" * 60)
    
    # Check for API key
    if not os.getenv("OPENAI_API_KEY"):
        print("âš ï¸ Warning: OPENAI_API_KEY not set")
        print("Set your API key: export OPENAI_API_KEY='your-key-here'")
        return
    
    # Run examples
    examples = [
        ("Basic Usage", basic_example),
        ("Advanced Configuration", advanced_example),
        ("Individual Agents", individual_agent_example),
        ("Error Handling", error_handling_example),
        ("Performance Testing", performance_example)
    ]
    
    results = {}
    
    for name, example_func in examples:
        print(f"\n{'='*60}")
        print(f"Running: {name}")
        print('='*60)
        
        try:
            result = await example_func()
            results[name] = "âœ… Success" if result else "âŒ Failed"
        except Exception as e:
            print(f"âŒ Example failed: {str(e)}")
            results[name] = f"âŒ Error: {str(e)}"
    
    # Summary
    print(f"\n{'='*60}")
    print("EXAMPLE SUMMARY")
    print('='*60)
    
    for name, status in results.items():
        print(f"{status} {name}")
    
    print(f"\nğŸ‰ Examples completed!")

if __name__ == "__main__":
    asyncio.run(main())
```

## ğŸ³ 5. Docker Configuration

### File: `docker-compose.yml`

```yaml
version: '3.8'

services:
  mcp-orchestrator:
    build: .
    container_name: mcp-qa-orchestrator
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GROK_API_KEY=${GROK_API_KEY}
      - LLM_PROVIDER=${LLM_PROVIDER:-openai}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - BDD_THRESHOLD=${BDD_THRESHOLD:-9.0}
      - QUALITY_THRESHOLD=${QUALITY_THRESHOLD:-8.0}
    volumes:
      - ./output:/app/output
      - ./logs:/app/logs
      - ./config:/app/config
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "python", "-c", "import mcp_qa_orchestrator; print('OK')"]
      interval: 30s
      timeout: 10s
      retries: 3

  mcp-monitor:
    build: .
    container_name: mcp-monitor
    command: python -m mcp_qa_orchestrator.monitor
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LOG_LEVEL=DEBUG
    volumes:
      - ./logs:/app/logs
    depends_on:
      - mcp-orchestrator
    restart: unless-stopped
```

### File: `Dockerfile`

```dockerfile
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create necessary directories
RUN mkdir -p output logs

# Set environment variables
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import mcp_qa_orchestrator; print('OK')"

# Default command
CMD ["python", "example_usage.py"]
```

## ğŸ”§ 6. Environment Template

### File: `.env.example`

```env
# LLM Configuration
OPENAI_API_KEY=your_openai_api_key_here
GROK_API_KEY=your_grok_api_key_here
LLM_PROVIDER=openai
LLM_BASE_URL=

# MCP Configuration
BDD_THRESHOLD=9.0
QUALITY_THRESHOLD=8.0
MAX_ITERATIONS=3
MAX_BUGS=3
TIMEOUT=30
RETRY_ATTEMPTS=3

# Logging Configuration
LOG_LEVEL=INFO
LOG_FILE=logs/mcp_orchestrator.log
LOG_MAX_SIZE=10MB
LOG_BACKUP_COUNT=5

# Output Configuration
OUTPUT_DIR=output
INCLUDE_TIMESTAMPS=true
COMPRESS_RESULTS=true
SAVE_INTERMEDIATE=true

# Quality Gates
MINIMUM_BDD_SCORE=8.0
MINIMUM_TEST_SCORE=7.0
MINIMUM_REVIEW_SCORE=7.0
MAXIMUM_BUGS=5
COVERAGE_THRESHOLD=80

# Agent Configuration
SCENARIO_BUILDER_MODEL=gpt-4
SCENARIO_BUILDER_TEMPERATURE=0.7
SCENARIO_BUILDER_MAX_TOKENS=3000

REVIEWER_MODEL=gpt-4
REVIEWER_TEMPERATURE=0.3
REVIEWER_MAX_TOKENS=2000

TEST_CODER_MODEL=gpt-4
TEST_CODER_TEMPERATURE=0.5
TEST_CODER_MAX_TOKENS=4000

CODE_REVIEWER_MODEL=gpt-4
CODE_REVIEWER_TEMPERATURE=0.2
CODE_REVIEWER_MAX_TOKENS=3000
```

## âœ… Success Criteria

1. **Configuration Files**:
   - [ ] requirements.txt with all dependencies
   - [ ] config.json with comprehensive settings
   - [ ] .env.example with all variables
   - [ ] docker-compose.yml for deployment

2. **Documentation**:
   - [ ] README.md with complete usage guide
   - [ ] example_usage.py with working examples
   - [ ] Installation and setup instructions
   - [ ] Troubleshooting and FAQ sections

3. **Deployment**:
   - [ ] Docker configuration working
   - [ ] Environment variable handling
   - [ ] Health checks implemented
   - [ ] Volume mounting configured

4. **Usability**:
   - [ ] Clear installation instructions
   - [ ] Working examples
   - [ ] Configuration options documented
   - [ ] Error handling examples

## ğŸš€ Implementation Notes

- **Follow Alicia's patterns**: Use similar configuration and documentation patterns
- **Comprehensive examples**: Include real-world usage scenarios
- **Docker ready**: Full containerization support
- **Environment flexible**: Support for different deployment environments
- **User friendly**: Clear documentation and examples

## ğŸ”— Integration Points

This Phase 3 implementation should integrate with:
- **Phase 1**: Core orchestrator and base classes
- **Phase 2**: LangChain agent implementations
- **Phase 4**: Testing and validation

The system should be production-ready after Phase 3, with complete configuration, documentation, and deployment support.

---

**Ready for Cline to implement!** ğŸš€
